{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "34ViK7GWmAxa",
        "gLJh_OJOmfdn"
      ],
      "authorship_tag": "ABX9TyOmpB6eAxTEpLG9+peDA2W1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcmachicao/pucp_energy__erasmus_modulos/blob/main/pucp_erasmus__RL_for_EnergyStorage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinformcement Learning Modeling for Energy Storage System"
      ],
      "metadata": {
        "id": "huv9H-4MYpPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Modeling"
      ],
      "metadata": {
        "id": "InDIQwvMYpF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces"
      ],
      "metadata": {
        "id": "E5e6aPo4WjwU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatteryStorageEnv(gym.Env):\n",
        "    def __init__(self, capacity=100, max_charge_rate=30, efficiency=0.95):\n",
        "        super(BatteryStorageEnv, self).__init__()\n",
        "\n",
        "        self.capacity = capacity\n",
        "        self.max_charge_rate = max_charge_rate\n",
        "        self.efficiency = efficiency\n",
        "\n",
        "        self.action_space = spaces.Discrete(3)  # -1: discharge, 0: idle, 1: charge\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0]),\n",
        "            high=np.array([capacity, 1000, 24, 1000]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        action_map = {0: -1, 1: 0, 2: 1}\n",
        "        action = action_map[action]\n",
        "\n",
        "        current_charge = self.state[0]\n",
        "        current_price = self.state[1]\n",
        "\n",
        "        # Calculate charge amount\n",
        "        charge_amount = action * self.max_charge_rate\n",
        "        if action > 0:  # Charging\n",
        "            charge_amount *= self.efficiency\n",
        "\n",
        "        # Don't allow discharge if battery is empty or charge if battery is full\n",
        "        if (current_charge <= 0 and action < 0) or (current_charge >= self.capacity and action > 0):\n",
        "            charge_amount = 0\n",
        "\n",
        "        new_charge = np.clip(current_charge + charge_amount, 0, self.capacity)\n",
        "\n",
        "        # Simple reward structure:\n",
        "        # Scale trading reward down\n",
        "        trading_reward = -charge_amount * current_price * 0.1\n",
        "\n",
        "        # Progressive boundary penalty\n",
        "        if new_charge <= 0:\n",
        "            boundary_penalty = -500\n",
        "        elif new_charge >= self.capacity:\n",
        "            boundary_penalty = -500\n",
        "        elif new_charge < 0.2 * self.capacity:\n",
        "            boundary_penalty = -100 * (0.2 - new_charge/self.capacity)\n",
        "        elif new_charge > 0.8 * self.capacity:\n",
        "            boundary_penalty = -100 * (new_charge/self.capacity - 0.8)\n",
        "        else:\n",
        "            boundary_penalty = 0\n",
        "\n",
        "        # Larger range reward\n",
        "        if 0.4 <= new_charge/self.capacity <= 0.8:\n",
        "            range_reward = 200\n",
        "        else:\n",
        "            range_reward = 0\n",
        "\n",
        "        reward = trading_reward + boundary_penalty + range_reward\n",
        "\n",
        "        # 2. Fixed penalty for being at 0% or 100%\n",
        "        if new_charge <= 0 or new_charge >= self.capacity:\n",
        "            boundary_penalty = -100  # High penalty for being at boundaries, it was -1000 at the beginning\n",
        "        else:\n",
        "            boundary_penalty = 0\n",
        "\n",
        "        # 3. Reward for being in good range (40-80%)\n",
        "        if 40 <= new_charge <= 80:\n",
        "            range_reward = 100\n",
        "        else:\n",
        "            range_reward = 0\n",
        "\n",
        "        reward = trading_reward + boundary_penalty + range_reward\n",
        "\n",
        "        # Update time and get new price\n",
        "        self.hour = (self.hour + 1) % 24\n",
        "        new_price = self._get_price()\n",
        "        new_demand = self._get_demand()\n",
        "\n",
        "        self.state = np.array([\n",
        "            new_charge,\n",
        "            new_price,\n",
        "            self.hour,\n",
        "            new_demand\n",
        "        ])\n",
        "\n",
        "        done = self.hour == 0\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.hour = 0\n",
        "        self.state = np.array([\n",
        "            self.capacity * 0.6,  # Start at 60% charge\n",
        "            self._get_price(),\n",
        "            self.hour,\n",
        "            self._get_demand()\n",
        "        ])\n",
        "        return self.state\n",
        "\n",
        "    def _get_price(self):\n",
        "        \"\"\"Simulated electricity price based on time of day\"\"\"\n",
        "        base_price = 50\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 30\n",
        "        return max(base_price + time_factor + np.random.normal(0, 5), 0)\n",
        "\n",
        "    def _get_demand(self):\n",
        "        \"\"\"Simulated demand based on time of day\"\"\"\n",
        "        base_demand = 500\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 200\n",
        "        return max(base_demand + time_factor + np.random.normal(0, 20), 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gye3Ejq3RLS2",
        "outputId": "06b3110c-44a4-44ba-c845-cb77a46bfd3c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Otras versiones de BatteryStorageEnv"
      ],
      "metadata": {
        "id": "34ViK7GWmAxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BatteryStorageEnv_v1(gym.Env):\n",
        "    def __init__(self, capacity=100, max_charge_rate=30, efficiency=0.95):\n",
        "        super(BatteryStorageEnv, self).__init__()\n",
        "\n",
        "        self.capacity = capacity\n",
        "        self.max_charge_rate = max_charge_rate\n",
        "        self.efficiency = efficiency\n",
        "\n",
        "        self.action_space = spaces.Discrete(3)  # -1: discharge, 0: idle, 1: charge\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0]),\n",
        "            high=np.array([capacity, 1000, 24, 1000]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Target range for battery level (40-80% is ideal operating range)\n",
        "        self.target_min = 0.4 * capacity\n",
        "        self.target_max = 0.8 * capacity\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        action_map = {0: -1, 1: 0, 2: 1}\n",
        "        action = action_map[action]\n",
        "\n",
        "        current_charge = self.state[0]\n",
        "        current_price = self.state[1]\n",
        "\n",
        "        # Calculate charge amount\n",
        "        charge_amount = action * self.max_charge_rate\n",
        "        if action > 0:  # Charging\n",
        "            charge_amount *= self.efficiency\n",
        "\n",
        "        # Don't allow discharge if battery is empty or charge if battery is full\n",
        "        if (current_charge <= 0 and action < 0) or (current_charge >= self.capacity and action > 0):\n",
        "            charge_amount = 0\n",
        "\n",
        "        new_charge = np.clip(current_charge + charge_amount, 0, self.capacity)\n",
        "\n",
        "        # Calculate rewards\n",
        "\n",
        "        # 1. Basic energy arbitrage reward (scaled down to be comparable with other rewards)\n",
        "        energy_reward = -charge_amount * current_price * 0.1\n",
        "\n",
        "        # 2. Penalty for being outside target range (quadratic penalty)\n",
        "        if new_charge < self.target_min:\n",
        "            level_penalty = -((self.target_min - new_charge) / self.capacity) ** 2 * 1000\n",
        "        elif new_charge > self.target_max:\n",
        "            level_penalty = -((new_charge - self.target_max) / self.capacity) ** 2 * 1000\n",
        "        else:\n",
        "            level_penalty = 0\n",
        "\n",
        "        # 3. Reward for maintaining good charge level\n",
        "        if self.target_min <= new_charge <= self.target_max:\n",
        "            maintenance_reward = 50\n",
        "        else:\n",
        "            maintenance_reward = 0\n",
        "\n",
        "        # 4. Severe penalty for complete discharge or overcharge\n",
        "        if new_charge <= 0 or new_charge >= self.capacity:\n",
        "            extreme_penalty = -2000\n",
        "        else:\n",
        "            extreme_penalty = 0\n",
        "\n",
        "        # Combine all rewards\n",
        "        reward = energy_reward + level_penalty + maintenance_reward + extreme_penalty\n",
        "\n",
        "        # Update time and get new price\n",
        "        self.hour = (self.hour + 1) % 24\n",
        "        new_price = self._get_price()\n",
        "        new_demand = self._get_demand()\n",
        "\n",
        "        self.state = np.array([\n",
        "            new_charge,\n",
        "            new_price,\n",
        "            self.hour,\n",
        "            new_demand\n",
        "        ])\n",
        "\n",
        "        done = self.hour == 0\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.hour = 0\n",
        "        self.state = np.array([\n",
        "            self.capacity * 0.6,  # Start at 60% charge\n",
        "            self._get_price(),\n",
        "            self.hour,\n",
        "            self._get_demand()\n",
        "        ])\n",
        "        return self.state\n",
        "\n",
        "    def _get_price(self):\n",
        "        \"\"\"Simulated electricity price based on time of day\"\"\"\n",
        "        base_price = 50\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 30\n",
        "        return max(base_price + time_factor + np.random.normal(0, 5), 0)\n",
        "\n",
        "    def _get_demand(self):\n",
        "        \"\"\"Simulated demand based on time of day\"\"\"\n",
        "        base_demand = 500\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 200\n",
        "        return max(base_demand + time_factor + np.random.normal(0, 20), 0)"
      ],
      "metadata": {
        "id": "gzNADoIde-O7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nhYiIvG-V3Dt"
      },
      "outputs": [],
      "source": [
        "class BatteryStorageEnv_v0(gym.Env):\n",
        "    \"\"\"\n",
        "    A simplified battery storage environment for RL\n",
        "    State space: [current_charge, current_price, time_of_day, current_demand]\n",
        "    Action space: [-1, 0, 1] (discharge, do nothing, charge)\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity=100, max_charge_rate=30, efficiency=0.95):\n",
        "        super(BatteryStorageEnv, self).__init__()\n",
        "\n",
        "        # Battery parameters\n",
        "        self.capacity = capacity\n",
        "        self.max_charge_rate = max_charge_rate\n",
        "        self.efficiency = efficiency\n",
        "\n",
        "        # Define action and observation spaces\n",
        "        self.action_space = spaces.Discrete(3)  # -1: discharge, 0: idle, 1: charge\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0]),  # charge, price, time, demand\n",
        "            high=np.array([capacity, 1000, 24, 1000]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        # Initialize state\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Decode action\n",
        "        action_map = {0: -1, 1: 0, 2: 1}\n",
        "        action = action_map[action]\n",
        "\n",
        "        # Get current state values\n",
        "        current_charge = self.state[0]\n",
        "        current_price = self.state[1]\n",
        "\n",
        "        # Update battery state\n",
        "        charge_amount = action * self.max_charge_rate\n",
        "        if action > 0:  # Charging\n",
        "            charge_amount *= self.efficiency\n",
        "\n",
        "        new_charge = np.clip(\n",
        "            current_charge + charge_amount,\n",
        "            0,\n",
        "            self.capacity\n",
        "        )\n",
        "\n",
        "        # Calculate primary reward (from energy arbitrage)\n",
        "        energy_reward = -charge_amount * current_price  # Negative when charging, positive when discharging\n",
        "\n",
        "        # Add penalty for very low battery levels (below 20%)\n",
        "        low_battery_penalty = 0\n",
        "        if new_charge < (0.2 * self.capacity):\n",
        "            low_battery_penalty = -50 * (0.2 - (new_charge / self.capacity))\n",
        "\n",
        "        # Add small penalty for very high battery levels (above 90%) to encourage utilization\n",
        "        high_battery_penalty = 0\n",
        "        if new_charge > (0.9 * self.capacity):\n",
        "            high_battery_penalty = -10 * ((new_charge / self.capacity) - 0.9)\n",
        "\n",
        "        # Combine rewards\n",
        "        reward = energy_reward + low_battery_penalty + high_battery_penalty\n",
        "\n",
        "        # Update time and get new price\n",
        "        self.hour = (self.hour + 1) % 24\n",
        "        new_price = self._get_price()\n",
        "        new_demand = self._get_demand()\n",
        "\n",
        "        # Update state\n",
        "        self.state = np.array([\n",
        "            new_charge,\n",
        "            new_price,\n",
        "            self.hour,\n",
        "            new_demand\n",
        "        ])\n",
        "\n",
        "        # Episode ends after 24 hours\n",
        "        done = self.hour == 0\n",
        "\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.hour = 0\n",
        "        self.state = np.array([\n",
        "            self.capacity * 0.5,  # Start at 50% charge\n",
        "            self._get_price(),\n",
        "            self.hour,\n",
        "            self._get_demand()\n",
        "        ])\n",
        "        return self.state\n",
        "\n",
        "    def _get_price(self):\n",
        "        \"\"\"Simulated electricity price based on time of day\"\"\"\n",
        "        base_price = 50\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 30\n",
        "        return max(base_price + time_factor + np.random.normal(0, 5), 0)\n",
        "\n",
        "    def _get_demand(self):\n",
        "        \"\"\"Simulated demand based on time of day\"\"\"\n",
        "        base_demand = 500\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 200\n",
        "        return max(base_demand + time_factor + np.random.normal(0, 20), 0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatteryStorageEnv_v3(gym.Env):\n",
        "    def __init__(self, capacity=100, max_charge_rate=30, efficiency=0.95):\n",
        "        super(BatteryStorageEnv, self).__init__()\n",
        "\n",
        "        self.capacity = capacity\n",
        "        self.max_charge_rate = max_charge_rate\n",
        "        self.efficiency = efficiency\n",
        "\n",
        "        self.action_space = spaces.Discrete(3)  # -1: discharge, 0: idle, 1: charge\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0]),\n",
        "            high=np.array([capacity, 1000, 24, 1000]),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        action_map = {0: -1, 1: 0, 2: 1}\n",
        "        action = action_map[action]\n",
        "\n",
        "        current_charge = self.state[0]\n",
        "        current_price = self.state[1]\n",
        "\n",
        "        # Calculate charge amount\n",
        "        charge_amount = action * self.max_charge_rate\n",
        "        if action > 0:  # Charging\n",
        "            charge_amount *= self.efficiency\n",
        "\n",
        "        # Don't allow discharge if battery is empty or charge if battery is full\n",
        "        if (current_charge <= 0 and action < 0) or (current_charge >= self.capacity and action > 0):\n",
        "            charge_amount = 0\n",
        "\n",
        "        new_charge = np.clip(current_charge + charge_amount, 0, self.capacity)\n",
        "\n",
        "        # Simple reward structure:\n",
        "        # 1. Profit/loss from energy trading\n",
        "        trading_reward = -charge_amount * current_price\n",
        "\n",
        "        # 2. Fixed penalty for being at 0% or 100%\n",
        "        if new_charge <= 0 or new_charge >= self.capacity:\n",
        "            boundary_penalty = -100  # High penalty for being at boundaries, it was -1000 at the beginning\n",
        "        else:\n",
        "            boundary_penalty = 0\n",
        "\n",
        "        # 3. Reward for being in good range (40-80%)\n",
        "        if 40 <= new_charge <= 80:\n",
        "            range_reward = 100\n",
        "        else:\n",
        "            range_reward = 0\n",
        "\n",
        "        reward = trading_reward + boundary_penalty + range_reward\n",
        "\n",
        "        # Update time and get new price\n",
        "        self.hour = (self.hour + 1) % 24\n",
        "        new_price = self._get_price()\n",
        "        new_demand = self._get_demand()\n",
        "\n",
        "        self.state = np.array([\n",
        "            new_charge,\n",
        "            new_price,\n",
        "            self.hour,\n",
        "            new_demand\n",
        "        ])\n",
        "\n",
        "        done = self.hour == 0\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.hour = 0\n",
        "        self.state = np.array([\n",
        "            self.capacity * 0.6,  # Start at 60% charge\n",
        "            self._get_price(),\n",
        "            self.hour,\n",
        "            self._get_demand()\n",
        "        ])\n",
        "        return self.state\n",
        "\n",
        "    def _get_price(self):\n",
        "        \"\"\"Simulated electricity price based on time of day\"\"\"\n",
        "        base_price = 50\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 30\n",
        "        return max(base_price + time_factor + np.random.normal(0, 5), 0)\n",
        "\n",
        "    def _get_demand(self):\n",
        "        \"\"\"Simulated demand based on time of day\"\"\"\n",
        "        base_demand = 500\n",
        "        time_factor = np.sin(self.hour * np.pi / 12) * 200\n",
        "        return max(base_demand + time_factor + np.random.normal(0, 20), 0)"
      ],
      "metadata": {
        "id": "yhlA9FCllv-I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Training the model"
      ],
      "metadata": {
        "id": "i5Fo_MugYfKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "J4yJVKzbYcxo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified Q-Learning agent with better exploration\n",
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, learning_rate=0.01, discount_factor=0.9, initial_epsilon=0.9):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(action_space.n))\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.initial_epsilon = initial_epsilon\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.action_space = action_space\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(0.01, self.initial_epsilon * (0.995 ** self.episode_count))\n",
        "        self.episode_count += 1\n",
        "\n",
        "    def get_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return self.action_space.sample()\n",
        "\n",
        "        state_tuple = tuple(np.round(state, 1))  # Round state values for better discretization\n",
        "        return np.argmax(self.q_table[state_tuple])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        state_tuple = tuple(np.round(state, 1))\n",
        "        next_state_tuple = tuple(np.round(next_state, 1))\n",
        "\n",
        "        best_next_action = np.argmax(self.q_table[next_state_tuple])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state_tuple][best_next_action]\n",
        "        td_error = td_target - self.q_table[state_tuple][action]\n",
        "        self.q_table[state_tuple][action] += self.lr * td_error"
      ],
      "metadata": {
        "id": "fC0qN9R2lnf1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Otras versiones QLearning"
      ],
      "metadata": {
        "id": "gLJh_OJOmfdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent_v1:\n",
        "    def __init__(self, action_space, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(action_space.n))\n",
        "        self.lr = learning_rate\n",
        "        self.gamma = discount_factor\n",
        "        self.epsilon = epsilon\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return self.action_space.sample()\n",
        "\n",
        "        # Convert state to tuple for dictionary key\n",
        "        state_tuple = tuple(state.astype(np.float32))\n",
        "        return np.argmax(self.q_table[state_tuple])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        # Convert states to tuples for dictionary keys\n",
        "        state_tuple = tuple(state.astype(np.float32))\n",
        "        next_state_tuple = tuple(next_state.astype(np.float32))\n",
        "\n",
        "        # Q-learning update\n",
        "        best_next_action = np.argmax(self.q_table[next_state_tuple])\n",
        "        td_target = reward + self.gamma * self.q_table[next_state_tuple][best_next_action]\n",
        "        td_error = td_target - self.q_table[state_tuple][action]\n",
        "        self.q_table[state_tuple][action] += self.lr * td_error"
      ],
      "metadata": {
        "id": "vpUAjrGhW5km"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "57wvsahHp20f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(env, agent, episodes=10000):\n",
        "    rewards_history = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.learn(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        rewards_history.append(total_reward)\n",
        "\n",
        "        # Print progress every 500 episodes\n",
        "        if (episode + 1) % 500 == 0:\n",
        "            avg_reward = np.mean(rewards_history[-100:])\n",
        "            print(f\"Episode {episode + 1}, Average Reward: {avg_reward:.2f}\")\n",
        "\n",
        "    return rewards_history"
      ],
      "metadata": {
        "id": "h4GXYPr_ZS-w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_results(rewards_history):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(rewards_history)\n",
        "    plt.title('Training Progress')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total Reward')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "PTuthxgoZJnv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create environment and agent\n",
        "env = BatteryStorageEnv()\n",
        "agent = QLearningAgent(env.action_space)"
      ],
      "metadata": {
        "id": "ectaLBdpaH22"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained agent\n",
        "def test_agent(env, agent, episodes=5):\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        actions_taken = []\n",
        "\n",
        "        print(f\"\\nEpisode {episode + 1}\")\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            actions_taken.append(action)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "            print(f\"Hour {int(state[2])}: Battery Level: {state[0]:.1f}%, \"\n",
        "                  f\"Price: ${state[1]:.2f}, Action: {['Discharge', 'Idle', 'Charge'][action]}\")\n",
        "\n",
        "        print(f\"Total Reward: ${total_reward:.2f}\")"
      ],
      "metadata": {
        "id": "vuav3MGTZmfs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test BEFORE training to show random behavior\n",
        "print(\"BEFORE TRAINING (Random Actions):\")\n",
        "test_agent(env, agent, episodes=2)  # Show just 2 episodes of bad performance\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTjTIJyiaLFg",
        "outputId": "41843e21-a268-4f8b-bb5b-fd5fcea505d9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE TRAINING (Random Actions):\n",
            "\n",
            "Episode 1\n",
            "Hour 1: Battery Level: 60.0%, Price: $58.03, Action: Idle\n",
            "Hour 2: Battery Level: 60.0%, Price: $64.81, Action: Idle\n",
            "Hour 3: Battery Level: 30.0%, Price: $71.40, Action: Discharge\n",
            "Hour 4: Battery Level: 0.0%, Price: $82.35, Action: Discharge\n",
            "Hour 5: Battery Level: 28.5%, Price: $79.39, Action: Charge\n",
            "Hour 6: Battery Level: 28.5%, Price: $77.51, Action: Idle\n",
            "Hour 7: Battery Level: 57.0%, Price: $80.84, Action: Charge\n",
            "Hour 8: Battery Level: 57.0%, Price: $75.74, Action: Idle\n",
            "Hour 9: Battery Level: 85.5%, Price: $80.80, Action: Charge\n",
            "Hour 10: Battery Level: 55.5%, Price: $62.90, Action: Discharge\n",
            "Hour 11: Battery Level: 55.5%, Price: $64.86, Action: Idle\n",
            "Hour 12: Battery Level: 55.5%, Price: $45.80, Action: Idle\n",
            "Hour 13: Battery Level: 84.0%, Price: $39.52, Action: Charge\n",
            "Hour 14: Battery Level: 100.0%, Price: $32.45, Action: Charge\n",
            "Hour 15: Battery Level: 70.0%, Price: $21.75, Action: Discharge\n",
            "Hour 16: Battery Level: 70.0%, Price: $21.87, Action: Idle\n",
            "Hour 17: Battery Level: 40.0%, Price: $16.47, Action: Discharge\n",
            "Hour 18: Battery Level: 68.5%, Price: $13.83, Action: Charge\n",
            "Hour 19: Battery Level: 97.0%, Price: $25.64, Action: Charge\n",
            "Hour 20: Battery Level: 97.0%, Price: $26.36, Action: Idle\n",
            "Hour 21: Battery Level: 100.0%, Price: $24.12, Action: Charge\n",
            "Hour 22: Battery Level: 70.0%, Price: $28.58, Action: Discharge\n",
            "Hour 23: Battery Level: 70.0%, Price: $49.22, Action: Idle\n",
            "Hour 0: Battery Level: 70.0%, Price: $55.06, Action: Idle\n",
            "Total Reward: $910.28\n",
            "\n",
            "Episode 2\n",
            "Hour 1: Battery Level: 60.0%, Price: $55.35, Action: Idle\n",
            "Hour 2: Battery Level: 30.0%, Price: $59.47, Action: Discharge\n",
            "Hour 3: Battery Level: 58.5%, Price: $66.18, Action: Charge\n",
            "Hour 4: Battery Level: 28.5%, Price: $71.03, Action: Discharge\n",
            "Hour 5: Battery Level: 0.0%, Price: $76.51, Action: Discharge\n",
            "Hour 6: Battery Level: 0.0%, Price: $75.18, Action: Idle\n",
            "Hour 7: Battery Level: 0.0%, Price: $74.91, Action: Idle\n",
            "Hour 8: Battery Level: 28.5%, Price: $70.36, Action: Charge\n",
            "Hour 9: Battery Level: 0.0%, Price: $78.44, Action: Discharge\n",
            "Hour 10: Battery Level: 0.0%, Price: $67.96, Action: Discharge\n",
            "Hour 11: Battery Level: 0.0%, Price: $58.06, Action: Discharge\n",
            "Hour 12: Battery Level: 0.0%, Price: $56.22, Action: Idle\n",
            "Hour 13: Battery Level: 0.0%, Price: $46.91, Action: Idle\n",
            "Hour 14: Battery Level: 28.5%, Price: $36.97, Action: Charge\n",
            "Hour 15: Battery Level: 0.0%, Price: $30.54, Action: Discharge\n",
            "Hour 16: Battery Level: 0.0%, Price: $25.15, Action: Idle\n",
            "Hour 17: Battery Level: 0.0%, Price: $22.10, Action: Discharge\n",
            "Hour 18: Battery Level: 28.5%, Price: $15.92, Action: Charge\n",
            "Hour 19: Battery Level: 57.0%, Price: $24.78, Action: Charge\n",
            "Hour 20: Battery Level: 57.0%, Price: $23.14, Action: Idle\n",
            "Hour 21: Battery Level: 57.0%, Price: $29.01, Action: Idle\n",
            "Hour 22: Battery Level: 85.5%, Price: $39.47, Action: Charge\n",
            "Hour 23: Battery Level: 55.5%, Price: $34.27, Action: Discharge\n",
            "Hour 0: Battery Level: 25.5%, Price: $46.87, Action: Discharge\n",
            "Total Reward: $-86.86\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now train the agent\n",
        "print(\"Training the agent...\")\n",
        "rewards_history = train_agent(env, agent, episodes=10000)\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB8MV39oXFYm",
        "outputId": "e3afafc7-3b01-4ba7-d748-f5b6027a197f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the agent...\n",
            "Episode 500, Average Reward: -159.72\n",
            "Episode 1000, Average Reward: -86.14\n",
            "Episode 1500, Average Reward: -198.05\n",
            "Episode 2000, Average Reward: -75.37\n",
            "Episode 2500, Average Reward: -80.80\n",
            "Episode 3000, Average Reward: -128.45\n",
            "Episode 3500, Average Reward: -124.34\n",
            "Episode 4000, Average Reward: -125.64\n",
            "Episode 4500, Average Reward: -192.91\n",
            "Episode 5000, Average Reward: -232.58\n",
            "Episode 5500, Average Reward: -120.62\n",
            "Episode 6000, Average Reward: -28.35\n",
            "Episode 6500, Average Reward: -168.50\n",
            "Episode 7000, Average Reward: -26.00\n",
            "Episode 7500, Average Reward: -84.86\n",
            "Episode 8000, Average Reward: -157.68\n",
            "Episode 8500, Average Reward: -57.23\n",
            "Episode 9000, Average Reward: -150.52\n",
            "Episode 9500, Average Reward: -250.68\n",
            "Episode 10000, Average Reward: -228.42\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test AFTER training to show improvement\n",
        "print(\"AFTER TRAINING (Learned Strategy):\")\n",
        "test_agent(env, agent, episodes=2)  # Show same number of episodes for comparison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5_5YKaybilE",
        "outputId": "3a829910-29e0-4fc0-dff8-d27a690cfc09"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AFTER TRAINING (Learned Strategy):\n",
            "\n",
            "Episode 1\n",
            "Hour 1: Battery Level: 30.0%, Price: $61.14, Action: Discharge\n",
            "Hour 2: Battery Level: 58.5%, Price: $63.96, Action: Charge\n",
            "Hour 3: Battery Level: 87.0%, Price: $79.85, Action: Charge\n",
            "Hour 4: Battery Level: 87.0%, Price: $87.49, Action: Idle\n",
            "Hour 5: Battery Level: 100.0%, Price: $72.92, Action: Charge\n",
            "Hour 6: Battery Level: 70.0%, Price: $80.29, Action: Discharge\n",
            "Hour 7: Battery Level: 40.0%, Price: $83.48, Action: Discharge\n",
            "Hour 8: Battery Level: 10.0%, Price: $78.64, Action: Discharge\n",
            "Hour 9: Battery Level: 0.0%, Price: $76.59, Action: Discharge\n",
            "Hour 10: Battery Level: 0.0%, Price: $63.92, Action: Discharge\n",
            "Hour 11: Battery Level: 0.0%, Price: $54.29, Action: Discharge\n",
            "Hour 12: Battery Level: 0.0%, Price: $53.19, Action: Discharge\n",
            "Hour 13: Battery Level: 0.0%, Price: $42.91, Action: Discharge\n",
            "Hour 14: Battery Level: 0.0%, Price: $35.56, Action: Idle\n",
            "Hour 15: Battery Level: 28.5%, Price: $28.37, Action: Charge\n",
            "Hour 16: Battery Level: 0.0%, Price: $25.06, Action: Discharge\n",
            "Hour 17: Battery Level: 28.5%, Price: $26.59, Action: Charge\n",
            "Hour 18: Battery Level: 57.0%, Price: $22.89, Action: Charge\n",
            "Hour 19: Battery Level: 27.0%, Price: $27.46, Action: Discharge\n",
            "Hour 20: Battery Level: 0.0%, Price: $20.50, Action: Discharge\n",
            "Hour 21: Battery Level: 0.0%, Price: $34.96, Action: Idle\n",
            "Hour 22: Battery Level: 28.5%, Price: $32.48, Action: Charge\n",
            "Hour 23: Battery Level: 57.0%, Price: $38.14, Action: Charge\n",
            "Hour 0: Battery Level: 85.5%, Price: $49.89, Action: Charge\n",
            "Total Reward: $-313.49\n",
            "\n",
            "Episode 2\n",
            "Hour 1: Battery Level: 30.0%, Price: $59.29, Action: Discharge\n",
            "Hour 2: Battery Level: 0.0%, Price: $67.85, Action: Discharge\n",
            "Hour 3: Battery Level: 0.0%, Price: $68.21, Action: Idle\n",
            "Hour 4: Battery Level: 0.0%, Price: $77.15, Action: Idle\n",
            "Hour 5: Battery Level: 28.5%, Price: $79.17, Action: Charge\n",
            "Hour 6: Battery Level: 57.0%, Price: $76.93, Action: Charge\n",
            "Hour 7: Battery Level: 27.0%, Price: $80.74, Action: Discharge\n",
            "Hour 8: Battery Level: 0.0%, Price: $70.32, Action: Discharge\n",
            "Hour 9: Battery Level: 0.0%, Price: $79.18, Action: Discharge\n",
            "Hour 10: Battery Level: 0.0%, Price: $64.73, Action: Discharge\n",
            "Hour 11: Battery Level: 28.5%, Price: $55.91, Action: Charge\n",
            "Hour 12: Battery Level: 57.0%, Price: $51.65, Action: Charge\n",
            "Hour 13: Battery Level: 85.5%, Price: $48.41, Action: Charge\n",
            "Hour 14: Battery Level: 55.5%, Price: $33.30, Action: Discharge\n",
            "Hour 15: Battery Level: 84.0%, Price: $28.74, Action: Charge\n",
            "Hour 16: Battery Level: 84.0%, Price: $22.28, Action: Idle\n",
            "Hour 17: Battery Level: 84.0%, Price: $25.04, Action: Idle\n",
            "Hour 18: Battery Level: 100.0%, Price: $17.89, Action: Charge\n",
            "Hour 19: Battery Level: 70.0%, Price: $16.73, Action: Discharge\n",
            "Hour 20: Battery Level: 40.0%, Price: $28.72, Action: Discharge\n",
            "Hour 21: Battery Level: 68.5%, Price: $32.08, Action: Charge\n",
            "Hour 22: Battery Level: 68.5%, Price: $36.45, Action: Idle\n",
            "Hour 23: Battery Level: 38.5%, Price: $28.47, Action: Discharge\n",
            "Hour 0: Battery Level: 38.5%, Price: $43.86, Action: Idle\n",
            "Total Reward: $-9.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comments"
      ],
      "metadata": {
        "id": "dUf0AOX5V4M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prompt: why do you think the Env is choosing to discharge to 0% and keeps the reward too low?\n",
        "\n",
        "The environment's tendency to discharge to 0% and maintain low rewards stems from a combination of factors in your reward structure and the agent's learning process.  Let's break down the likely culprits:\n",
        "\n",
        "1. **Reward Structure Imbalance:**\n",
        "\n",
        "   * **High Penalty for Boundaries:** The `boundary_penalty` of -1000 for hitting 0% or 100% is extremely severe.  This overwhelms other rewards. The agent learns very quickly to avoid these states *at all costs*, even if it means sacrificing potential profit.  It's prioritizing avoiding the massive penalty over maximizing profit.\n",
        "\n",
        "   * **Trading Reward is too low:**  The `trading_reward`  might not be significant enough to outweigh the huge boundary penalty, especially in early stages of training. The agent's exploration might lead it to situations where small trading profits are overshadowed by the risk of hitting the boundaries.\n",
        "\n",
        "\n",
        "2. **Exploration-Exploitation Dilemma:**\n",
        "\n",
        "    * **Insufficient Exploration:** If the agent's exploration rate (`epsilon`) is too low, it might get stuck in a local optimum where discharging is a \"safe\" strategy, even if it's not the most profitable one.  Early on, it needs to explore the state space more thoroughly to find better strategies.\n",
        "\n",
        "    * **Epsilon Decay:** Ensure your agent's `epsilon` decays gradually during training.  A constant epsilon might prevent the agent from exploiting its learned knowledge.\n",
        "\n",
        "3. **Learning Rate and Discount Factor:**\n",
        "\n",
        "    * **Learning Rate:** The learning rate (`lr`) might be too high or too low. Too high and the agent might oscillate and not converge. Too low and the agent might learn very slowly.\n",
        "    * **Discount Factor:** The discount factor (`gamma`) is related to how much the agent values future rewards. A very high gamma might lead the agent to focus heavily on long-term rewards, potentially making it less sensitive to immediate penalties.\n",
        "\n",
        "\n",
        "**Debugging and Improvement Strategies:**\n",
        "\n",
        "1. **Reward Engineering:**\n",
        "   * **Scale Down the Boundary Penalty:** Reduce `boundary_penalty` to something less drastic, perhaps -10 or -50.  The goal is to discourage boundary hits, not to make them catastrophic.\n",
        "   * **Scale Up the Trading Reward:** Increase the magnitude of `trading_reward` so that the agent sees significant gains when making profitable energy trading decisions.  You might need to experiment with scaling factors or modifying how the trading reward is calculated.\n",
        "   * **Smooth Boundary Penalty (Optional):** Instead of a sharp penalty, consider a quadratic penalty that increases as the charge level gets close to the boundaries.  This would create a smoother incentive for staying within the acceptable range.\n",
        "\n",
        "2. **Exploration:**\n",
        "   * **Higher Initial Epsilon:** Start with a higher `epsilon` (e.g., 0.5 or 0.7) to encourage more exploration in the initial training stages.\n",
        "   * **Decaying Epsilon:** Implement an appropriate decay schedule for `epsilon`, so it gradually decreases over the training episodes, allowing the agent to exploit learned policies later on.\n",
        "\n",
        "3. **Parameter Tuning:**\n",
        "   * **Learning Rate:** Experiment with smaller values of `lr` (e.g., 0.01) to stabilize learning.\n",
        "   * **Discount Factor:** A moderate discount factor (e.g., 0.95) is often a good starting point.\n",
        "\n",
        "\n",
        "4. **State Discretization:**\n",
        "    * You use `np.round(state, 1)` in your agent.  The granularity of this rounding impacts the discretization of your state space.  Consider reducing the rounding value or using another discretization technique if this is causing issues.\n",
        "\n",
        "5. **Printing intermediate states:** Add printing to show the different components of your reward function during training, which would help in visualizing if your rewards are behaving correctly.\n",
        "\n",
        "6. **Debug Initial State and Pricing:** Verify that the initial price and state of the battery are favorable, and that prices are indeed fluctuating.\n",
        "\n",
        "7. **Alternative RL Algorithms:**  You could test other RL algorithms like DQN or SARSA.  They might find better strategies.\n",
        "\n",
        "\n",
        "\n",
        "By carefully adjusting these parameters, you should be able to improve the agent's learning and prevent it from constantly discharging to 0%. Remember to test different combinations of values until you find what works well for your environment.\n"
      ],
      "metadata": {
        "id": "A_nn_sTKowdx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFhXJxRkbihu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Added a target operating range (40-80% of capacity)\n",
        "Created multiple reward components:\n",
        "\n",
        "Scaled down the energy arbitrage reward\n",
        "Added quadratic penalty for being outside target range\n",
        "Added positive reward for staying within target range\n",
        "Added severe penalty for complete discharge/overcharge\n",
        "\n",
        "\n",
        "Made rewards more balanced relative to each other\n",
        "Start battery at 60% charge instead of 50%\n",
        "Prevented impossible actions (can't discharge empty battery or charge full battery)"
      ],
      "metadata": {
        "id": "ZlL7SYszey9b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GBhXDRGqP7qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def step(self, action):\n",
        "    # ... existing code ...\n",
        "\n",
        "    # Scale trading reward down\n",
        "    trading_reward = -charge_amount * current_price * 0.1\n",
        "\n",
        "    # Progressive boundary penalty\n",
        "    if new_charge <= 0:\n",
        "        boundary_penalty = -500\n",
        "    elif new_charge >= self.capacity:\n",
        "        boundary_penalty = -500\n",
        "    elif new_charge < 0.2 * self.capacity:\n",
        "        boundary_penalty = -100 * (0.2 - new_charge/self.capacity)\n",
        "    elif new_charge > 0.8 * self.capacity:\n",
        "        boundary_penalty = -100 * (new_charge/self.capacity - 0.8)\n",
        "    else:\n",
        "        boundary_penalty = 0\n",
        "\n",
        "    # Larger range reward\n",
        "    if 0.4 <= new_charge/self.capacity <= 0.8:\n",
        "        range_reward = 200\n",
        "    else:\n",
        "        range_reward = 0\n",
        "\n",
        "    reward = trading_reward + boundary_penalty + range_reward"
      ],
      "metadata": {
        "id": "wCBjzBMWP7P4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}